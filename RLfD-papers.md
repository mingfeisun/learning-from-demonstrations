# Robot Learning From Demonstration: Domain Overview and Classification

* Author: Ziming Chen
* Created: 2018-08-06
* Updated: 2018-08-10

Overview of the field and detailed classification of related paper in the corresponding repository.

### Domain overview

* [A survey of robot learning from demonstration](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1022.4510&rep=rep1&type=pdf)
 
  Brenna D. Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. Robotics and Autonomous systems 57, no. 5, pp. 469-483, 2009. 

* [Robot learning from demonstration](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.54.8531&rep=rep1&type=pdf)

  Atkeson, Christopher G., and Stefan Schaal. 
  "Robot learning from demonstration." ICML. Vol. 97. 1997.
  
* [Robot programming by Demonstration](https://infoscience.epfl.ch/record/114050/files/chapter5_9.pdf)

  Billard, Aude, et al. "Robot programming by demonstration." Springer handbook of robotics. Springer, Berlin, Heidelberg, 2008. 1371-1394.  
  
* [Robot Learning from Human Teachers](https://www.morganclaypool.com/doi/abs/10.2200/S00568ED1V01Y201402AIM028) ***

    Chernova S, Thomaz A L. Robot learning from human teachers[J]. Synthesis Lectures on Artificial Intelligence and Machine Learning, 2014, 8(3): 1-121.
  
### Reward shaping

* [Policy invariance under reward transformations: theory and application to reward shaping](https://people.eecs.berkeley.edu/%7Epabbeel/cs287-fa09/readings/NgHaradaRussell-shaping-ICML1999.pdf)

    Ng, Andrew Y., Daishi Harada, and Stuart Russell. "Policy invariance under reward transformations: Theory and application to reward shaping." ICML. Vol. 99. 1999.
    
* [Reinforcement Learning from Demonstration through Shaping](https://ijcai.org/Proceedings/15/Papers/472.pdf)

    Brys, Tim, et al. "Reinforcement Learning from Demonstration through Shaping." IJCAI. 2015.
    
* [Introspective Reinforcement Learning and Learning from Demonstration](http://ala2018.it.nuigalway.ie/papers/ALA_2018_paper_5.pdf) ***
 
    Li, Mao, Tim Brys, and Daniel Kudenko. "Introspective Reinforcement Learning and Learning from Demonstration." Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems. International Foundation for Autonomous Agents and Multiagent Systems, 2018.
    
* [Learning from Demonstration for Shaping through Inverse Reinforcement Learning](http://delivery.acm.org/10.1145/2940000/2936988/p429-suay.pdf?ip=175.159.124.168&id=2936988&acc=ACTIVE%20SERVICE&key=CDD1E79C27AC4E65%2EFC30B8D6EF32B758%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1533653137_c97b941806e4a837ff232c73c2f34a6f) ***

    Suay, Halit Bener, et al. "Learning from demonstration for shaping through inverse reinforcement learning." Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems. International Foundation for Autonomous Agents and Multiagent Systems, 2016.    
    
### Transfer Learning

* [A survey on transfer learning](https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf)

    Pan, Sinno Jialin, and Qiang Yang. "A survey on transfer learning." IEEE Transactions on knowledge and data engineering 22.10 (2010): 1345-1359.

* [Transfer learning for reinforcement learning domains: A survey](http://www.jmlr.org/papers/volume10/taylor09a/taylor09a.pdf)

    Taylor, Matthew E., and Peter Stone. "Transfer learning for reinforcement learning domains: A survey." Journal of Machine Learning Research 10.Jul (2009): 1633-1685.
    
* [Shaping in reinforcement learning by knowledge transferred from human-demonstrations of a simple similar task](https://content.iospress.com/download/journal-of-intelligent-and-fuzzy-systems/ifs17052?id=journal-of-intelligent-and-fuzzy-systems%2Fifs17052)

    Wang, Guo-Fang, Zhou Fang, and Ping Li. "Shaping in reinforcement learning by knowledge transferred from human-demonstrations of a simple similar task." Journal of Intelligent & Fuzzy Systems 34.1 (2018): 711-720.


### Trajectory imitation

* [Learning and generalization of motor skills by learning from demonstration](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F15/papers/Pastor09.pdf)

  Pastor, Peter, et al. "Learning and generalization of motor skills by learning from demonstration." Robotics and Automation, 2009. ICRA'09. IEEE International Conference on. IEEE, 2009.

* [Online movement adaptation based on previous sensor experiences](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/PastorForce.pdf)

  Peter Pastor, Ludovic Righetti, Mrinal Kalakrishnan, and Stefan Schaal. IEEE/RSJ International Conference on Intelligent Robots and Systems, 2011.

###  Keyframe imitation

* [Keyframe-based learning from demonstration](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/AkgunKey.pdf)
    
    Baris Akgun, Maya Cakmak, Karl Jiang, and Andrea L. Thomaz.
    International Journal of Social Robotics, 2012\.

* [Trajectories and keyframes for kinesthetic teaching: A human-robot interaction perspective](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/Akgun12.pdf)
    
    Baris Akgun, Maya Cakmak, Jae Wook Yoo, and Andrea Lockerd Thomaz.
    ACM/IEEE International Conference on Human-Robot Interaction, 2012\.

### Supervised learning

* [A reduction of imitation learning and structured prediction to no-regret online learning](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/Ross11.pdf)
    
    Stéphane Ross, Geoffrey J. Gordon, and J. Andrew Bagnell.
    arXiv preprint arXiv:1011.0686, 2011.

* [On learning, representing, and generalizing a task in a humanoid robot](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/Calinon07.pdf)
    
    Sylvain Calinon, Florent Guenter, and Aude Billard.
    IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, 37(2), 2007.

### Reinforcement learning

* [Reinforcement learning in robotics: A survey](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/Kober13.pdf)
    
    Jens Kober, J. Andrew Bagnell, and Jan Peters.
    International Journal of Robotics Research, 2013\.

* [Improving Reinforcement Learning with Human Input](http://www.ijcai.org/proceedings/2018/0817.pdf) ***

    Taylor, Matthew E., and A. I. Borealis. "Improving Reinforcement Learning with Human Input." IJCAI. 2018.   

### Deep Reinforcement Learning
    
* [Human-level control through deep reinforcement learning
](http://www.davidqiu.com:8888/research/nature14236.pdf) ***

    Mnih, Volodymyr, et al. "Human-level control through deep reinforcement learning." Nature 518.7540 (2015): 529.
    
* [Deep Reinforcement Learning with Double Q-Learning](http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847) ***

    Van Hasselt, Hado, Arthur Guez, and David Silver. "Deep Reinforcement Learning with Double Q-Learning." AAAI. Vol. 2. 2016.
    
* [Deep Q-learning from Demonstrations](https://arxiv.org/pdf/1704.03732.pdf)

    Hester, Todd, et al. "Deep Q-learning from Demonstrations." arXiv preprint arXiv:1704.03732 (2017).

* [Deep Reinforcement Learning from Human Preferences](https://papers.nips.cc/paper/7017-deep-reinforcement-learning-from-human-preferences.pdf) ***

    Christiano, Paul F., et al. "Deep reinforcement learning from human preferences." Advances in Neural Information Processing Systems(NIPS). 2017.
    
* [Guided cost learning: Deep inverse optimal control via policy optimization](http://proceedings.mlr.press/v48/finn16.pdf)

    Finn, Chelsea, Sergey Levine, and Pieter Abbeel. "Guided cost learning: Deep inverse optimal control via policy optimization." International Conference on Machine Learning. 2016.

* [Deep reinforcement learning: An overview](https://arxiv.org/pdf/1701.07274.pdf)
    
    Li, Yuxi. "Deep reinforcement learning: An overview." arXiv preprint arXiv:1701.07274 (2017).
    
### Policy search I

* [Skill learning and task outcome prediction for manipulation](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/Pastor11.pdf)
    
    Peter Pastor, Mrinal Kalakrishnan, Sachin Chitta, Evangelos Theodorou, and Stefan Schaal.
    IEEE International Conference on Robotics and Automation, 2011\.

*  [Learning concurrent motor skills in versatile solution spaces](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/HIREPS.pdf)
    
    Christian Daniel, Gerhard Neumann, and Jan Peters.
    IEEE/RSJ International Conference on Intelligent Robots and Systems, 2012\.

### Policy search II

* [Learning contact-rich manipulation skills with guided policy search](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/Levine15.pdf)
    
    Sergey Levine, Nolan Wagener, and Pieter Abbeel.
    arXiv preprint arXiv:1501.05611, 2015\.

* [PILCO: A model-based and data-efficient approach to policy search](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/Deisenroth11.pdf)
    
    Marc Deisenroth and Carl E. Rasmussen.
    International Conference on Machine Learning, 2011\.

### Inverse reinforcement learning I

* [Inverse reinforcement learning through structured classification](http://papers.nips.cc/paper/4551-inverse-reinforcement-learning-through-structured-classification.pdf) ***

    Klein, Edouard, et al. "Inverse reinforcement learning through structured classification." Advances in Neural Information Processing Systems(NIPS). 2012.

* [Apprenticeship learning via inverse reinforcement learning](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/Abbeel04.pdf)
 
    Pieter Abbeel and Andrew Y. Ng.
    International Conference on Machine learning, 2004\.

* [Bayesian inverse reinforcement learning](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/BayesIRL.pdf)
    
    Deepak Ramachandran and Eyal Amir.
    International Joint Conference on Artificial Intelligence, 2007.
    
* [Algorithms for inverse reinforcement learning](http://ai.stanford.edu/~ang/papers/icml00-irl.pdf)

    Ng, Andrew Y., and Stuart J. Russell. 
    ICML, 2000.
    
### Inverse reinforcement learning II  

* [Maximum entropy inverse reinforcement learning](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/Ziebart08.pdf)

    Ziebart, Brian D., Andrew L. Maas, J. Andrew Bagnell, and Anind K. Dey.
    AAAI Conference on Artificial Intelligence, 2008\.

* [Maximum Entropy Deep Inverse Reinforcement Learning](https://arxiv.org/pdf/1507.04888.pdf) ***

    Wulfmeier, M., Ondruska, P., and Posner, I. Maximum entropy deep inverse reinforcement learning. arXiv preprint arXiv:1507.04888, 2015.

* [Relative Entropy Inverse Reinforcement Learning](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/relent.pdf)
    
    Abdeslam Boularias, Jens Kober, and Jan Peters.
    AISTATS, 2011\.

### Optimality in IRL

* [Algorithmic and human teaching of sequential decision tasks](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/CakmakIRL.pdf)
    
    Maya Cakmak and Manuel Lopes.
    AAAI Conference on Artificial Intelligence, 2012\.

* [Cooperative Inverse Reinforcement Learning](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/CIRL.pdf)
    
    Dylan Hadfield-Menell, Anca Dragan, Pieter Abbeel, and Stuart Russell.
    Neural Information Processing Systems, 2016\.

### Learning to plan

* [Demonstration-Guided Motion Planning](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/DemoRRT.pdf)
    
    Gu Ye and Ron Alterovitz.
    International Symposium on Robotics Research, 2011\.

* [Learning to plan for constrained manipulation from demonstrations](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/Phillips13.pdf)
 
    Mike Phillips, Victor Hwang, Sachin Chitta, and Maxim Likhachev.
    Robotics: Science and Systems, 2013\.
    
* [Socially Adaptive Path Planning in Human Environments Using Inverse Reinforcement Learning](https://link.springer.com/article/10.1007/s12369-015-0310-2) ***

    Kim, Beomjoon, and Joelle Pineau. "Socially adaptive path planning in human environments using inverse reinforcement learning." International Journal of Social Robotics 8.1 (2016): 51-66.

* [Learning Objective Functions for Manipulation](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6630743)

    Kalakrishnan, Mrinal, et al. "Learning objective functions for manipulation." Robotics and Automation (ICRA), 2013 IEEE International Conference on. IEEE, 2013.
    
* [Socially Compliant Mobile Robot Navigation via Inverse Reinforcement Learning](http://www.cs.cmu.edu/~jeanoh/16-785/papers/kretzschmar-ijrr2016-socially.pdf)

    Kretzschmar, Henrik, et al. "Socially compliant mobile robot navigation via inverse reinforcement learning." The International Journal of Robotics Research 35.11 (2016): 1289-1307.

### Affordance learning

* [Affordance-based imitation learning in robots](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/Lopes07.pdf)
    
    Manuel Lopes, Francisco S. Melo, and Luis Montesano.
    IEEE/RSJ International Conference on Intelligent Robots and Systems, 2007\.

* [Learning visual object definitions by observing human activities](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/Veloso05.pdf)
    
    Manuela Veloso, Felix Von Hundelshausen, and Paul E. Rybski.
    IEEE-RAS International Conference on Humanoid Robots, 2005\.

### Skill learning I

* [Incremental semantically grounded learning from demonstration](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/Niekum13.pdf)
    
    Scott Niekum, Sachin Chitta, Andrew G. Barto, Bhaskara Marthi, and Sarah Osentoski.
    Robotics: Science and Systems, 2013\.

* [A brief introduction to bayesian nonparametric methods for clustering and time series analysis](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/NiekumTR2015.pdf)
    
    Scott Niekum.
    Technical report CMU-RI-TR-15-02, Robotics Institute, Carnegie Mellon University, 2015\.

### Skill learning II

* [Towards learning hierarchical skills for multi-phase manipulation tasks](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/KroemerSTARHMM.pdf)
  
    Oliver Kroemer, Christian Daniel, Gerhard Neumann, Herke Van Hoof, and Jan Peters.
    IEEE International Conference on Robotics and Automation, 2015\.

* [Transition State Clustering: Unsupervised Surgical Trajectory Segmentation For Robot Learning](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/TSCIJRR.pdf)
    
    Sanjay Krishnan, Animesh Garg, Sachin Patil, Colin Lea, Gregory Hager, Pieter Abbeel, Ken Goldberg.
    Submitted to The International Journal of Robotics Research. May 2016\.

### Learning object kinematics  

* [Online bayesian changepoint detection for articulated motion models](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/Niekum15.pdf)
  
    Scott Niekum, Sarah Osentoski, Christopher G. Atkeson, and Andrew G. Barto.
    IEEE International Conference on Robotics and Automation, 2015\.

* [Active articulation model estimation through interactive perception](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/Hausman15.pdf)
  
    Karol Hausman, Scott Niekum, Sarah Osentoski, and G. Sukhatme.
    IEEE International Conference on Robotics and Automation, 2015\.

### Active learning I

* [Designing Interactions for Robot Active Learners](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5471105) ***

    Cakmak, Maya, Crystal Chao, and Andrea L. Thomaz. "Designing interactions for robot active learners." IEEE Transactions on Autonomous Mental Development 2.2 (2010): 108-118.
    
*   [Interactive policy learning through confidence-based autonomy](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/Chernova09.pdf)
    
    Sonia Chernova and Manuela Veloso.
    Journal of Artificial Intelligence Research, 2009\.

*   [Designing robot learners that ask good questions](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/DemoQuery.pdf)
    
    Maya Cakmak and Andrea L. Thomaz.
    ACM/IEEE International Conference on Human-Robot Interaction, 2012\.
    
* [Active Learning for Reward Estimation in Inverse Reinforcement Learning](http://robotcub.org/misc/papers/09_Lopes_Melo_etal_ecml.pdf) ***

    Lopes, Manuel, Francisco Melo, and Luis Montesano. "Active learning for reward estimation in inverse reinforcement learning." Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, Berlin, Heidelberg, 2009.

* [Improving Reinforcement Learning with Confidence-Based Demonstrations](https://pdfs.semanticscholar.org/5851/82e93bc33daacfc7cf52744eae569e180c52.pdf) ***

    Wang, Zhaodong, and Matthew E. Taylor. "Improving reinforcement learning with confidence-based demonstrations." Proceedings of the 26th International Conference on Artificial Intelligence (IJCAI). 2017.
    
* [Interactive Reinforcement Learning with Dynamic Reuse of Prior Knowledge from Human/Agent's Demonstration](https://arxiv.org/pdf/1805.04493.pdf) **[optional]**

    Wang, Zhaodong, and Matthew E. Taylor. "Interactive Reinforcement Learning with Dynamic Reuse of Prior Knowledge from Human/Agent's Demonstration." arXiv preprint arXiv:1805.04493 (2018).
    
### Active learning II / Information gathering actions I

* [Active reward learning](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/ActiveReward.pdf)

  Daniel, Christian, et al. "Active Reward Learning." Robotics: Science and Systems. 2014.

* [Information Gathering Actions over Human Internal State](https://dorsa.fyi/publications/sadigh2016information.pdf)

  Dorsa Sadigh, et al. "Information gathering actions over human internal state." Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on. IEEE, 2016.

* [Active Preference-Based Learning of Reward Functions](https://dorsa.fyi/publications/sadigh2017active.pdf) ***

  Dorsa Sadigh, Anca D. Dragan, Shankar Sastry, and Sanjit A. Seshia. "Active preference-based learning of reward functions." Robotics: Science and Systems (RSS). 2017.
  
### Information gathering actions II

*   [Belief space planning assuming maximum likelihood observations](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/Platt10.pdf)
    
    Robert Platt Jr., Russ Tedrake, Leslie Kaelbling, and Tomas Lozano-Perez.
    Robotics: Science and Systems, 2010\.

*   [Efficient touch based localization through submodularity](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/SubmodTouch.pdf)
    
    Shervin Javdani, Matthew Klingensmith, J. Andrew Bagnell, Nancy S. Pollard, and Siddhartha S. Srinivasa.
    IEEE International Conference on Robotics and Automation, 2013\.

### Dialog I

*  [Understanding natural language commands for robotic navigation and mobile manipulation](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/Tellex11.pdf)
    
    Stefanie Tellex, Thomas Kollar, Steven Dickerson, Matthew R. Walter, Ashis Gopal Banerjee, Seth J. Teller, and Nicholas Roy.
    AAAI Conference on Artificial Intelligence, 2011\.

*   [Asking for help using inverse semantics](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/Tellex14.pdf)
    
    Stefanie Tellex, Ross Knepper, Adrian Li, Daniela Rus, and Nicholas Roy.
    Robotics: Science and Systems, 2014\.

### Dialog II

*   [Learning to Interpret Natural Language Commands through Human-Robot Dialog](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/IncrementalParse.pdf)
    
    Jesse Thomason, Shiqi Zhang, Raymond Mooney, and Peter Stone
    International Joint Conference on Artificial Intelligence, 2015\.

*   [Tell me Dave: context-sensitive grounding of natural language to mobile manipulation instructions](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/Misra14.pdf)
    
    Dipendra K Misra, Jaeyong Sung, Kevin Lee, Ashutosh Saxena
    Robotics: Science and Systems, 2014\.

### Interactive learning I

*   [Interactively shaping agents via human reinforcement: The TAMER framework.](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/Knox09.pdf)
    
    W. Bradley Knox and Peter Stone.
    International Conference on Knowledge Capture, 2009\.

*   [Training a robot via human feedback: A case study](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/Knox13.pdf)
    
    W. Bradley Knox, Peter Stone, and Cynthia Breazeal.
    International Conference on Social Robotics, 2013\.

* [Combining Manual Feedback with Subsequent MDP Reward Signals for Reinforcement Learning](http://delivery.acm.org/10.1145/1840000/1838208/p5-knox.pdf?ip=175.159.124.168&id=1838208&acc=ACTIVE%20SERVICE&key=CDD1E79C27AC4E65%2EFC30B8D6EF32B758%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1533819242_38f7d685a0cc2005344deed10fd79d94) ***

    Knox, W. Bradley, and Peter Stone. Proceedings of the 9th International Conference on Autonomous Agents and Multi-agent Systems, 2010.
    
* [A Need for Speed: Adapting Agent Action Speed to Improve Task Learning from Non-Expert Humans](http://delivery.acm.org/10.1145/2940000/2937065/p957-peng.pdf?ip=175.159.124.168&id=2937065&acc=ACTIVE%20SERVICE&key=CDD1E79C27AC4E65%2EFC30B8D6EF32B758%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1533717988_fbe25a65c57742d0bbda0483e076d6fd) ***

    Peng, Bei, et al. "A need for speed: Adapting agent action speed to improve task learning from non-expert humans." Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems. International Foundation for Autonomous Agents and Multiagent Systems, 2016.
    
* [Framing reinforcement learning from human reward: Reward positivity, temporal discounting, episodicity, and performance](http://www.dtic.mil/get-tr-doc/pdf?AD=ADA627123)

    Knox, W. Bradley, and Peter Stone. "Framing reinforcement learning from human reward: Reward positivity, temporal discounting, episodicity, and performance." Artificial Intelligence, 2015.
    
### Interactive learning II

*   [Learning trajectory preferences for manipulators via iterative improvement](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/Jain13.pdf)
    
    Ashesh Jain, Brian Wojcik, Thorsten Joachims, and Ashutosh Saxena.
    Advances in Neural Information Processing Systems, 2013\.

*   [Robot Programming from Demonstration, Feedback and Transfer](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/InteractGUI.pdf)
    
    Yoan Mollard, Thibaut Munzer, Andrea Baisero, Marc Toussaint, and Manuel Lopes.
    International Conference on Intelligent Robots and Systems, 2015\.

* [Natural methods for robot task learning: instructive demonstrations, generalization and practice](http://delivery.acm.org/10.1145/870000/860614/p241-nicolescu.pdf?ip=175.159.124.168&id=860614&acc=ACTIVE%20SERVICE&key=CDD1E79C27AC4E65%2EFC30B8D6EF32B758%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1533630747_a436bfaa692ccc199ff23a5ca34728bb) 

  Nicolescu, Monica N., and Maja J. Mataric. "Natural methods for robot task learning: Instructive demonstrations, generalization and practice." Proceedings of the second international joint conference on Autonomous agents and multiagent systems. ACM, 2003.

### Shared autonomy

*   [Formalizing assistive teleop](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/AssistTele.pdf)
    
    Anca Dragan and Siddhartha Srinivasa.
    Robotics: Science and Systems, 2012

*   [Human-in-the-Loop Optimization of Shared Autonomy in Assistive Robotics](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/ArgallSA.pdf)

    Deepak Gopinath, Siddarth Jain, and Brenna D. Argall.
    IEEE Robotics and Automation Letters, 2016 

### Human factors I

*   [Generating legible motion](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/Dragan13.pdf)
    
    Anca Dragan and Siddhartha Srinivasa.
    Robotics: Science and Systems, 2013\.

*   [Human-robot cross-training: computational formulation, modeling and evaluation of a human team training strategy](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/Nikolaidis13.pdf) ***
      
    Stefanos Nikolaidis and Julie Shah.
    ACM/IEEE International Conference on Human-Robot Interaction, 2013\.
    
* [Improved human–robot team performance through cross-training, an approach inspired by human team training practices](https://www.ri.cmu.edu/pub_files/2015/11/The-International-Journal-of-Robotics-Research-2015-Nikolaidis-1711-30.pdf) ***

    Nikolaidis, Stefanos, et al. The International Journal of Robotics Research, 2015

    
* [Leveraging Social Networks to Motivate Humans to Train Agents](http://delivery.acm.org/10.1145/2620000/2616067/p1571-li.pdf?ip=175.159.124.168&id=2616067&acc=ACTIVE%20SERVICE&key=CDD1E79C27AC4E65%2EFC30B8D6EF32B758%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1533721682_57bc5510c950f3fe1fb809de1bfe0f3b) ***

    Li, Guangliang, et al. Proceedings of the 2014 international conference on autonomous agents and multi-agent systems, 2014.
    
* [Using Informative Behavior to Increase Engagement in the TAMER Framework](http://bradknox.net/public/papers/aamas13-li.pdf) ***

    Li, Guangliang, et al. Proceedings of the 2013 international conference on Autonomous agents and multi-agent systems, 2013.

* [Social interaction for efficient agent learning from human reward](https://link.springer.com/article/10.1007/s10458-017-9374-8) ***
    
    Li, Guangliang, et al. "Social interaction for efficient agent learning from human reward." Autonomous Agents and Multi-Agent Systems 32.1 (2018): 1-25.
    
### Human factors II

*   [Trust calibration within a human-robot team: Comparing automatically generated explanations](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/Trust.pdf)
    
    Ning Wang, David V. Pynadath, and Susan G. Hill.
    ACM/IEEE International Conference on Human-Robot Interaction, 2016\.

*   [Anticipating human activities using object affordances for reactive robotic response](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/Koppula13.pdf)
    
    Hema Koppula and Ashutosh Saxena.
    Robotics: Science and Systems, 2013\.

### Safety

*   [High-Confidence Off-Policy Evaluation](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/Thomas2015.pdf)
    
    Philip S. Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh.
    AAAI Conference on Artificial Intelligence, 2015\.

*   [High Confidence Off-Policy Evaluation with Models](http://www.cs.utexas.edu/~sniekum/classes/RLFD-F16/papers/Hanna2016.pdf)
    
    Josiah P. Hanna, Peter Stone, Scott Niekum.
    arXiv:1606.06126, June 2016\.
